Whether the pressure is pinned or not, the velocity profile is identical. That's
a plus.

However, the profiles are different depending on whether the pressure is
integrated by parts or not.

So the oscillations are not caused by the convective term in the NS momentum
equation. It's definitely caused by the radial boundary condition at the axis of
symmetry.

Fantastic description of different classes of boundary conditions:
http://www.math.ttu.edu/~klong/Sundance/html/bcs.html

Well this is interesting; the _coord_type gets selected correctly when I run the
debug executable, but incorrectly when I run the opt executable.

I'm finding what Martin Leseur found: integrating p by parts creates
problems. Ok, there was a problem with the sign in the do nothing BC. When that
sign is fixed, the open flow problem is solved correctly in cartesian
coordinates whether the pressure is integrated by parts or not. The open flow
problem is solved correctly in cylindrical coordinates if the pressure is not
integrated by parts; but currently it does not converge to a solution if the
pressure is integrated by parts.

Tomorrow, record eigenvalue and flux values for different choices of the
postprocessor. Understand why the choice doesn't matter.

bxnorm = regular norm

+----------------+----------------+----------------+----------------+----------------+----------------+----------------+
| time           | bnorm          | eigenvalue     | group1max      | group1norm     | group2max      | group2norm     |
+----------------+----------------+----------------+----------------+----------------+----------------+----------------+
|   0.000000e+00 |   1.000000e+00 |   1.000000e+00 |   0.000000e+00 |   8.590069e+01 |   0.000000e+00 |   8.590069e+01 |
|   1.000000e+00 |   1.827927e+00 |   1.827927e+00 |   1.804033e-01 |   3.188402e+02 |   1.215683e-01 |   1.672426e+02 |
|   2.000000e+00 |   1.828075e+00 |   1.828075e+00 |   1.804071e-01 |   3.188541e+02 |   1.215784e-01 |   1.672560e+02 |
+----------------+----------------+----------------+----------------+----------------+----------------+----------------+

bxnorm = group1 norm

Postprocessor Values:
+----------------+----------------+----------------+----------------+----------------+----------------+----------------+
| time           | bnorm          | eigenvalue     | group1max      | group1norm     | group2max      | group2norm     |
+----------------+----------------+----------------+----------------+----------------+----------------+----------------+
|   0.000000e+00 |   1.164135e-02 |   1.000000e+00 |   0.000000e+00 |   1.000000e+00 |   0.000000e+00 |   1.000000e+00 |
|   1.000000e+00 |   1.049054e-02 |   1.828668e+00 |   1.034865e-03 |   1.828668e+00 |   6.967703e-04 |   9.598150e-01 |
|   2.000000e+00 |   1.048084e-02 |   1.828075e+00 |   1.034322e-03 |   1.828075e+00 |   6.970413e-04 |   9.589231e-01 |
+----------------+----------------+----------------+----------------+----------------+----------------+----------------+

bxnorm = group2 norm

Postprocessor Values:
+----------------+----------------+----------------+----------------+----------------+----------------+----------------+
| time           | bnorm          | eigenvalue     | group1max      | group1norm     | group2max      | group2norm     |
+----------------+----------------+----------------+----------------+----------------+----------------+----------------+
|   0.000000e+00 |   1.164135e-02 |   1.000000e+00 |   0.000000e+00 |   1.000000e+00 |   0.000000e+00 |   1.000000e+00 |
|   1.000000e+00 |   1.998091e-02 |   1.828113e+00 |   1.971931e-03 |   3.485134e+00 |   1.328793e-03 |   1.828113e+00 |
|   2.000000e+00 |   1.998051e-02 |   1.828075e+00 |   1.971815e-03 |   3.485013e+00 |   1.328828e-03 |   1.828075e+00 |
+----------------+----------------+----------------+----------------+----------------+----------------+----------------+

Notes on preconditioning:

asm-lu pre, full=false: linear its = 20
asm-lu pre, full=true: linear its = 17
default pre, full=true: linear its = 11
default pre, full=fase: linear its = 11

Ok, _grad_zero is of type std::vector<VariableGradient>. The length of the
vector is equal to the number of threads. Each element of the vector is created
like this: _grad_zero[tid].resize(getMaxQps(), RealGradient(0.));
- The type of VariableGradient is MooseArray<RealGradient>. The array has length
  equal to the number of quadrature points in the geometric element. And each
  element is of type RealGradient which is of type RealVectorValue.
- How are RealVectorValues initialized?
- RealVectorValue is of type VectorValue<Real>
- And VectorValue is a templated class

Environments for compiling on cray:

- module load cray-petsc
- module load cray-hdf5
- module switch PrgEnv-cray PrgEnv-gcc
- module load cray-mpich

So with a full preconditioner, it took 6 nonlinear iterations with PJFNK (it
does not converge with NEWTON within 50 nonlinear iterations). 7 nonlinear
iterations with just the diagonal components on the preconditioner...moreover
the number of linear iterations required was quite a bit higher. It also took 6
nonlinear iterations with line_search turned back on with PJFNK and with the
full preconditioner.

### 11/14/16

axisymm_cylinder.geo:

- H = 39.6 cm: not multiplying
- H = 80 cm; multiplying
- H = 60 cm; not multiplying
- H = 70 cm; not multiplying

There is a monotonic decrease in group 1 and group 2 fission and NSF cross sections with
increasing temperature. However, there's also a monotonic decrease in the
removal cross section as well. Diffusion constants generally trend downward with
temperature in the fuel. Same in the moderator.

Precursor1 is always the residual that causes the simulation to fail. It gets
this absolutely ridiculous depression in its concentration in the middle of the
reactor. If I look a few time steps before the simulation starts crashing, then
the precursor concentration looks like it should: monotonic. It's worthwhile to
note that precursor1 isn't the only precursor whose behavior goes from monotonic
to oscillatory; about half the precursor groups become oscillatory by the end of
failed simulations. So my job is to figure out what's going on. Why does it
crash? The problem is not:

- the RZ formulation
- the precursor decay kernels

The problem can't be fixed by:
- just reducing the advection velocity
- changing prec scaling from 1e-3 to 1e3
- increasing artificial diffusion

When the precursor simulation fails, the temperature and flux profiles look
fine.

Can definitely fix it with a sufficient source stabilization...but then the
problem becomes accuracy.

Order of tasks:

deprecated_block
finish_input_file_output
meta_action
no_action
setup_oversampling

Order of tasks that do things:
- dynamic_object_registration
    - Problem
- common_output
    - Outputs
- set_global_params
    - GlobalParams
- setup_recover_file_base
- check_copy_nodal_vars
    - temp
- setup_mesh
    - Mesh


So it makes sense that it takes so much bloody time for heat to diffuse from the
surface of the moderator into the moderator bulk. The charateristic diffusion
time from interface to wall is 42 seconds. The massive amounts of heat from
fission are being generated over the course of hundredths of a second. So yea
we're going to have to go to some wall like condition at the interface...e.g. we
cannot have Tmod = Tfuel at the interface.

Coefficient of thermal expansion, graphite: 2-6e-6 m/(m*K)

Doubling fluid flow velocity (see leastSquared directory), had absolutely no
impact on the maximum temperature in the reactor which seems a little bit odd to
me.

Putting those dumb boundary conditions in place for the temperature in the
moderator produces a hideous spectrum. I think how Cammi got around these things
is because he never used local temperatures to determine his properties. He used
block averaged values. So he could integrate over half the moderator domain to
determine the average temperature and then use that for the neutron group
constants in the other half.

Working on monotone cubic interpolation:

The second order accurate finite difference methods for derivative computation
will compute derivatives exactly for a second order polynomial. They WILL NOT
correctly compute derivatives for a third order polynomial because they are only
second order accurate. The FC algorithm initializes derivatives using this
second order differencing scheme. However, the FB algorithm roughly averages
neighboring secant lines...this is not second order accurate (or is it?).

Let's say we have some function. We want to evaluate/estimate it's derivative at
a point. How can we do that? We can perform a finite difference to do the
estimation:

f'(x) ~ (f(x+h) - f(x-h)) / h

We can write the error of this approximation as:

E(h) = C * h**n

where n is the order of accuracy. We can imagine a case where a method is nth
order accurate and the error is zero; this corresponds to C = 0.

From wikipedia: "The Newton series consists of the terms of the Newton forward
difference equation, named after Isaac Newton; in essence, it is the Newton
interpolation formula, namely the discrete analog of the continuum Taylor
expansion, which holds for any polynomial function f and for most (but not all)
analytic functions.

You can define your numerical derivative however you want! But then the key is
you need to be able to set-up an expression:

y' + Error(stuff, h) = numerical\_derivative

And then you can inspect the parts that make up the error in order to determine
it's functional dependence on h and perhaps whether the error is zero because of
other components that make up the error function. Cool!

yt is not correctly rendering any of my new neutronic simulation data sets! Need
to figure out why.

Ok first test was running a single region test with Quad4 elements. That test
went sucessfully which makes sense because Andrew has had a Q2 sampler
implemented forever.

It should be noted that data values in yt aren't actually accessed until
performing data access, something like `ad['diffused']`. Even this operation:
`ad = ds.all_data()` won't actually read the variable values.

Prism volumes:
second order volumes: 1.17718
first order volumes: 1.06066

For just one element:
first order volume: 1.5 (as it should be)
second order volume: 2.32843; very close to 2.356 which is equal to:
pi * r^2 * h / 4

The rule for computing the PRISM18 volume can be found in the libmesh source.

The natural boundary condition appears to have an effect on the flow at the
boundary for shizzle.

I've been able to get the flow simulation to work perfectly with the correct
geometry and the correct boundary conditions. Nice!

Ok, so when linking a library with -l, the linker searches in order from first
to last of directories provided with -L. -rpath matters for run-time; if there's
a mistake in supplying rpath during the linking stage, this error will show up
at run-time, NOT during program linking.

ldd uses LD_LIBRARY_PATH. You can kind of think of ldd as a command that
executes the shared object in order to discover its dependencies. Things that
get executed care about their environment; e.g. in this case ldd certainly cares
about LD_LIBRARY_PATH.

Changing LD_LIBRARY_PATH will change the memory location/library version that a
shared object points to. E.g. if I alter LD_LIBRARY_PATH, then I can change the
location of the library that libgfortran.so.3 points to.

When resolving shared object dependencies of an executable or other shared
object, the looking order is roughly the following:

1. Directories specified in the DT_RPATH dynamic section attribute of the
   executable/shared object in question (this is set at compile/linking time for
   example with -W,-rpath)
2. From the environment variable LD_LIBRARY_PATH
3. From the library directories listed in /etc/ld.so.conf
4. Finally in /lib and then /usr/lib

Nice!

The problematic file appears to be...kind of the core shared
object...libmesh_dbg.so

Successfull compilation command for the previously failing unit_tests-dbg:

ccache clang++ -std=gnu++11 -O0 -felide-constructors -g -pedantic -W -Wall
-Wextra -Wno-long-long -Wunused -Wpointer-arith -Wformat -Wparentheses
-Qunused-arguments -Woverloaded-virtual -fopenmp -std=gnu++11 -o
.libs/unit_tests-dbg hello.cpp -Wl,-rpath -Wl,/opt/moose/tbb/lib -Wl,-rpath
-Wl,/opt/moose/petsc/mpich_petsc-3.7.4/clang-opt-superlu/lib -Wl,-rpath
-Wl,/opt/moose/gcc-6.2.0/lib/gcc/x86_64-pc-linux-gnu/6.2.0 -Wl,-rpath
-Wl,/opt/moose/gcc-6.2.0/lib64 -Wl,-rpath -Wl,/lib/x86_64-linux-gnu -Wl,-rpath
-Wl,/opt/moose/gcc-6.2.0/lib -Wl,-rpath
-Wl,/opt/moose/mpich/mpich-3.2/clang-opt/lib -Wl,-rpath
-Wl,/opt/moose/llvm-3.9.0/lib -Wl,-rpath -Wl,/usr/lib/x86_64-linux-gnu
-L/opt/moose/petsc/mpich_petsc-3.7.4/clang-opt-superlu/lib
-L/opt/moose/mpich/mpich-3.2/clang-opt/lib -L/opt/moose/gcc-6.2.0/lib64
-L/opt/moose/gcc-6.2.0/lib
/home/lindsayad/test_multiple_petsc/libmesh/build/contrib/netcdf/v4/liblib/.libs/libnetcdf.so
/usr/lib/x86_64-linux-gnu/libcurl-gnutls.so -lz -L/opt/moose/tbb/lib -ltbb
-ltbbmalloc -lpetsc -lsuperlu_dist -lcmumps -ldmumps -lsmumps -lzmumps
-lmumps_common -lpord -lparmetis -lmetis -lHYPRE
-L/opt/moose/gcc-6.2.0/lib/gcc/x86_64-pc-linux-gnu/6.2.0 -L/lib/x86_64-linux-gnu
-L/opt/moose/llvm-3.9.0/lib -lscalapack -lflapack -lfblas -lX11 -lhwloc
/opt/moose/mpich/mpich-3.2/clang-opt/lib/libmpifort.so
/opt/moose/gcc-6.2.0/lib/../lib64/libgfortran.so
/opt/moose/gcc-6.2.0/lib/../lib64/libgomp.so
/opt/moose/gcc-6.2.0/lib/../lib64/libquadmath.so
/opt/moose/mpich/mpich-3.2/clang-opt/lib/libmpicxx.so
/opt/moose/gcc-6.2.0/lib/../lib64/libstdc++.so -lm
/opt/moose/mpich/mpich-3.2/clang-opt/lib/libmpi.so -lrt -lomp -lgcc_s -lpthread
-L/usr/lib/x86_64-linux-gnu -L/opt/moose/cppunit-1.12.1/clang-opt/lib -lcppunit
-ldl -pthread -fopenmp -Wl,-rpath
-Wl,/home/lindsayad/test_multiple_petsc/scripts/../libmesh/installed/lib
-Wl,-rpath -Wl,/opt/moose/gcc-6.2.0/lib/../lib64 -Wl,-rpath
-Wl,/opt/moose/mpich/mpich-3.2/clang-opt/lib

Should it be enough...?

I learned exactly why the libmesh build was failing. The reason is that when
compiling and linking the libmesh_dbg library, the /usr/lib/x86_64-linux-gnu
directory was passed to the linker before the opt petsc directory, meaning that
-lpetsc links in the system petsc as opposed to the MOOSE environment petsc. And
then when compiling and linking the unit_tests-dbg shared object, the opt petsc directory
is passed first, so there's a mismatch there.

To summarize: system petsc can be used to build libmesh/moose as long as the
moose environment is not loaded. However, it's pretty much impossible to unload
the system packages, so the moose environment petsc cannot be used to build
libmesh/moose as long as the system petsc is installed.

Knowing why this failed is enough for me. There's really no practical use to
having two petsc installations loaded at any time.

Cannot source .bashrc when a conda virtualenv is active.

In typedef:

typedef <ITEM1> <ITEM2>

The second item is being defined to equal the first item, e.g., you can think of
it as:

ITEM2 = ITEM1

e.g. ITEM1 is the thing that already exists.

Looking at dof_map_constraints.C in libMesh:

During interpolation of the nodes, these dofs get fixed:

first element (0xa74eb0):
0
1
3
4
7
This must be a corner element of the global mesh

second element (0xa75020):
0
1
4
This must be a side element of the global mesh

So question is: how is it determined whether a degree of freedom is nodal or edge? A reasonable guess would be that a "nodal" degree of freedom corresponds to an element vertex.

Another question: why is it that on edge dofs the solution values aren't simply assigned to the evaluation of the dirichlet function?

On a quadrilateral side (with second order shape functions), there are three quadrature points. I'm curious where the quadrature points are located.

Ok, stripping the problem down to one element. Then the quadrature points of one example side are:

(-.77, -1, 0)
(0, -1, 0)
(.77, -1, 0)

Which of these also corresponds to a degree of freedom?? Why the point (0, -1, 0) does of course.

Solution value at (-.77, -1, 0): -.3467

Ok the solution values at the quadrature points calculated in libmesh match the dirichlet function being projected onto them. That's good!

I need to understand what an infinite homogeneous medium really means in the context of Scale.

Files that cause compilation errors during Scale build:

packages/Tsurfer/read_npt.f90:1552
packages/Senlib/read_response_M.f90:277
packages/Senlib/read_response_M.f90:394

Trying to get familiar with Scale execution. Currently performing an infinite lattice calculation with NEWT (sample newt1.inp).

Call sequence so far:

main (ScalePrograms/scale.cpp)
Scale::ScaleDriver::run (ScaleDriver/ScaleDriver.cpp)
Scale::ScaleDriver::runSequence (ScaleDriver/ScaleDriver.cpp)
Scale::Sequence::TNewt::execute (Sequence/TNewt/TNewt.cpp)
Scale::Module::BaseTMGTransport::execute (Module/Xsdrn/BaseTMGTransport.cpp)
Scale::Module::XSProcModule::execute (Module/XSProc/XSProcModule.cpp)

From whithin XSProceModule::execute we call:

Crawdad::execute (Crawdad/Crawdad.cpp)

amd then:

XSProc::execute (XSProc/XSProc/XSProc.cpp)

From within XSProc::excute we call:

Dancoff::execute (XSProc/dancoff/Dancoff.cpp)
execute_dancoff (XSProc/dancoff/DancoffInterface.cpp)
F_executeDancoff (XSProc/dancoff/Dancoff_I.f90)
execute_dancoff (XSProc/dancoff/Dancoff_M.f90)

and then:

bonamiM::execute (XSProc/bonamiM/nonamiM.cpp)

and then:

MixMacros::execute (XSProc/MixMacros/MixMacros.cpp)

and then:

CentrmPmc::execute (PMC/CentrmPmc.cpp)

and then:

MixMacros::execute

Once all that is finished we return to Scale::Module::BaseTMGTransport::execute (BaseTMGTransport.cpp) which then calls:

Scale::Module::NewtModule::execute (Module/Newt/NewtModule.cpp)

BaseTMGTransport::execute looks like an important function. It is the function from which both the execution routines for generating the multigroup cross sections and the actual transport code are called.

Here's Newt backtrace:

#0  newtapifortran_m::executenewt (inputsaver=..., lib=...)
    at /home/lindsayad/scale/SCALE-6.2-serial-6.2.1-Source/packages/Newt/NewtApiFortran.f90:184
#1  0x00007fffe8673d47 in newtapibinding_m::f_executenewt (
    inputsaver=<error reading variable: Attempt to dereference a generic pointer.>,
    libs=<error reading variable: Attempt to dereference a generic pointer.>)
    at /home/lindsayad/scale/SCALE-6.2-serial-6.2.1-Source/packages/Newt/NewtApiBinding.f90:38
#2  0x00007ffff2b88d0b in NEWT::NewtApi::executeNewt (this=0x7fffffffc130, inputSaver=0x5db0718, lib=0x238af770)
    at /home/lindsayad/scale/SCALE-6.2-serial-6.2.1-Source/packages/Newt/NewtApi.h:41
#3  0x00007ffff2b85689 in Scale::Module::NewtModule::execute (this=0x5db0660)
    at /home/lindsayad/scale/SCALE-6.2-serial-6.2.1-Source/packages/Module/Newt/NewtModule.cpp:212
#4  0x00007ffff293932b in Scale::Module::BaseTMGTransport::execute (this=0x68f710)
    at /home/lindsayad/scale/SCALE-6.2-serial-6.2.1-Source/packages/Module/Xsdrn/BaseTMGTransport.cpp:174
#5  0x00007ffff7206c94 in Scale::Sequence::TNewt::execute (this=0x68f470)
    at /home/lindsayad/scale/SCALE-6.2-serial-6.2.1-Source/packages/Sequence/TNewt/TNewt.cpp:200
#6  0x00007ffff7bc16a3 in Scale::ScaleDriver::runSequence (this=0x7fffffffd620, name=..., sequence=..., current_sequence_stream=...)
    at /home/lindsayad/scale/SCALE-6.2-serial-6.2.1-Source/packages/ScaleDriver/ScaleDriver.cpp:591
#7  0x00007ffff7bbfe6c in Scale::ScaleDriver::run(Standard::Factory<Standard::AbstractSequence*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::function<Standard::AbstractSequence* ()> >&) (this=0x7fffffffd620, sequenceFactory=...)
    at /home/lindsayad/scale/SCALE-6.2-serial-6.2.1-Source/packages/ScaleDriver/ScaleDriver.cpp:389
#8  0x000000000041919f in main (argc=2, argv=0x7fffffffd908)
    at /home/lindsayad/scale/SCALE-6.2-serial-6.2.1-Source/packages/ScalePrograms/scale.cpp:173

Now looking at executing my silly little input file.

Still call Crawdad and then again:

Dancoff
Bonami
MixMacros
CentrmPmc
MixMacros

Drilling down a little into centrmpmc.

Crawdad gets called according to the documentation because CentrmPmc requires it (or something like that).

Ok, using an infinite lattice as opposed to inifinite homogeneous medium, I still get high k values:

k-eff = 1.41823805
k-infinity from four factor formula = 1.419670

After "Critical Spectra Calculated with the B1 Method":

Critical Buckling = 1.05846e-3
k-infinity of critical system = 1.38594

Changing the grid size from 5x5 to 10x10 (at least I believe that's what I was doing) did not appreciably change the results.
